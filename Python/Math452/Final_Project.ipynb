{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLF409jI94Dg"
   },
   "source": [
    "# MATH 497: Final Project\n",
    "\n",
    "Remark: \n",
    "\n",
    "Please upload your solutions for this project to Canvas with a file named \"Final_Project_yourname.ipynb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2ENBj8Nz3kg"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUx4g1oZ94Di"
   },
   "source": [
    "## Problem 1 [20%]:  \n",
    "\n",
    "Consider the following linear system\n",
    "\n",
    "\\begin{equation}\\label{matrix}\n",
    "A\\ast u =f,\n",
    "\\end{equation}\n",
    "or equivalently $u=\\arg\\min \\frac{1}{2} (A* v,v)_F-(f,v)_F$, where $(f,v)_F =\\sum\\limits_{i,j=1}^{n}f_{i,j}v_{i,j}$ is the Frobenius inner product.\n",
    "Here $\\ast$ represents a convolution with one channel, stride one and zero padding one. The convolution kernel $A$ is given by\n",
    "$$ \n",
    "A=\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix},~~\n",
    "$$\n",
    "the solution $ u \\in \\mathbb{R}^{n\\times n} $, and the RHS $ f\\in \\mathbb{R}^{n\\times n}$ is given by $f_{i,j}=\\dfrac{1}{(n+1)^2}.$\n",
    "\n",
    "\n",
    "### Tasks:\n",
    "Set $J=4$, $n=2^J-1$ and the number of iterations $M=100$. Use the gradient descent method and the multigrid method to solve the above problem with a random initial guess $u^0$. Let $u_{GD}$ and $u_{MG}$ denote the solutions obtained by gradient descent and multigrid respectively.\n",
    "    \n",
    "* [5%] Plot the surface of solution $u_{GD}$ and $u_{MG}$.\n",
    "\n",
    "* [10%] Define error $e_{GD}^m = \\|A * u^{m}_{GD}- f\\|_F=\\sqrt{\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n} |(A * u^{m}_{GD}- f)_{i,j}}|^2 $ for $m=0,1,2,3,...,M$. Similarly, we define the multigrid error $e_{MG}^m$. Plot the errors $e_{GD}^m$ and $e_{MG}^m$ as a function of the iteration $m$ (your x-axis is $m$ and your y-axis is the error). Put both plots together in the same figure.\n",
    "\n",
    "* [5%] Find the minimal $m_1$ for which $e^{m_1}_{GD} <10^{-5}$ and the minimal $m_2$ for which $e^{m_2}_{MG} <10^{-5}$, and report the computational time for each method. Note that $m_1$ or $m_2$ may be greater than $M=100$, in this case you will have to run more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark:\n",
    "\n",
    "Below are examples of using gradient descent and multigrid iterations for M-times \n",
    "* #### For gradient descent method with $\\eta=\\frac{1}{8}$, you need to write a code:\n",
    "\n",
    "    Given initial guess $u^0$\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{for    }  m =  1,2,...,M\\\\\n",
    "&~~~~\\text{for    }  i,j = 1: n\\\\\n",
    "&~~~~~~~~u_{i,j}^{m} = u_{i,j}^{m-1}-\\eta(f_{i,j}-(A\\ast u^{m-1})_{i,j})\\\\\n",
    "&~~~~\\text{endfor}\\\\\n",
    "&\\text{endfor}\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "* #### For multigrid method, we have provided the framework code in F02_MultigridandMgNet.ipynb:\n",
    "\n",
    "    Given initial guess $u^0$\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{for    }  m =  1,2,...,M\\\\\n",
    "&~~~~u^{m} = MG1(u^{m-1},f, J, \\nu)\\\\\n",
    "&\\text{endfor}\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Use GPU?', use_cuda)\n",
    "    \n",
    "##### For MG: inilization of A, S, Pi, R, RT #####\n",
    "def get_mg_init(A=None, S=None, Pi=None, R=None, RT=None):\n",
    "\n",
    "    A_kernel = torch.tensor([[[[0,-1,0],[-1,4,-1],[0,-1,0]]]],dtype=torch.float32)\n",
    "    S_kernel = torch.tensor([[[[0,1/64,0],[1/64,12/64,1/64],[0,1/64,0]]]],dtype=torch.float32)\n",
    "    Pi_kernel = torch.tensor([[[[0,0,0],[0,0,0],[0,0,0]]]],dtype=torch.float32)\n",
    "    R_kernel = torch.tensor([[[[0,0.5,0.5],[0.5,1,0.5],[0.5,0.5,0]]]],dtype=torch.float32)\n",
    "    RT_kernel = torch.tensor([[[[0,0.5,0.5],[0.5,1,0.5],[0.5,0.5,0]]]],dtype=torch.float32)\n",
    "\n",
    "    if A is not None:\n",
    "        A.weight = torch.nn.Parameter(A_kernel)\n",
    "    \n",
    "    if S is not None:\n",
    "        S.weight = torch.nn.Parameter(S_kernel)\n",
    "        \n",
    "    if Pi is not None:\n",
    "        Pi.weight = torch.nn.Parameter(Pi_kernel)\n",
    "    \n",
    "    if R is not None:\n",
    "        R.weight = torch.nn.Parameter(R_kernel)\n",
    "    \n",
    "    if RT is not None:\n",
    "        RT.weight = torch.nn.Parameter(RT_kernel)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "##### For MG: setup for prolongation and error calculation #####\n",
    "RT = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=2, padding=0, bias=False)\n",
    "get_mg_init(None,None,None,None,RT)\n",
    "A = nn.Conv2d(1, 1, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "get_mg_init(A,None,None,None,None)\n",
    "\n",
    "class MgIte(nn.Module):\n",
    "    def __init__(self, A, S):\n",
    "        super().__init__()\n",
    "        \n",
    "        get_mg_init(A=A,S=S)                       ##### For MG: inilization of A, S #####\n",
    "\n",
    "        self.A = A\n",
    "        self.S = S\n",
    "    \n",
    "    def forward(self, out):\n",
    "        u, f = out\n",
    "        u = u + (self.S(((f-self.A(u)))))                                   ##### For MG: u = u + S*(f-A*u) #####\n",
    "        out = (u, f)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MgRestriction(nn.Module):\n",
    "    def __init__(self, A_old, A, Pi, R):\n",
    "        super().__init__()\n",
    "\n",
    "        get_mg_init(A=A,Pi=Pi,R=R)            ##### For MG: inilization of A, Pi, R #####\n",
    "\n",
    "        self.A_old = A_old\n",
    "        self.A = A\n",
    "        self.Pi = Pi\n",
    "        self.R = R\n",
    "        \n",
    "    def forward(self, out):\n",
    "        u_old, f_old = out\n",
    "        u = self.Pi(u_old)                              ##### For MG: u = Pi*u_old #####\n",
    "        f = self.R(f_old-self.A_old(u_old)) + self.A(u) ##### For MG: f = R*(f_old-A_old*u_old) + A*u #####\n",
    "        out = (u,f)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MG(nn.Module):\n",
    "    def __init__(self, num_channel_input, num_iteration, num_channel_u, num_channel_f, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_iteration = num_iteration\n",
    "        self.num_channel_u = num_channel_u\n",
    "        \n",
    "        A = nn.Conv2d(num_channel_u, num_channel_f, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "        layers = []\n",
    "        for l, num_iteration_l in enumerate(num_iteration): #l: l-th layer.   num_iteration_l: the number of iterations of l-th layer\n",
    "            for i in range(num_iteration_l):\n",
    "                layers.append(MgIte(A, S))\n",
    "\n",
    "            setattr(self, 'layer'+str(l), nn.Sequential(*layers))\n",
    "\n",
    "            if l < len(num_iteration)-1:\n",
    "                A_old = A\n",
    "                \n",
    "                A = nn.Conv2d(num_channel_u, num_channel_f, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "                S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "                ##### For MG: padding=0 #####  \n",
    "                Pi = nn.Conv2d(num_channel_u, num_channel_u, kernel_size=3,stride=2, padding=0, bias=False)\n",
    "                R  = nn.Conv2d(num_channel_f, num_channel_f, kernel_size=3, stride=2, padding=0, bias=False)\n",
    "                layers= [MgRestriction(A_old, A, Pi, R)]\n",
    "        \n",
    "    def forward(self, u, f):\n",
    "        out = (u, f) \n",
    "\n",
    "        u_list.append(u)                                      ##### For MG: save u^j, j=1,2,...,J #####\n",
    "        for l in range(len(self.num_iteration)):\n",
    "            out = getattr(self, 'layer'+str(l))(out) \n",
    "            u, f = out                                        ##### For MG: save u^j, j=1,2,...,J #####\n",
    "            u_list.append(u)                                  ##### For MG: save u^j, j=1,2,...,J #####\n",
    "        \n",
    "        return u                                        \n",
    "    \n",
    "'''\n",
    "MODEL SETUP & TESTING\n",
    "'''\n",
    "#Plotting functions \n",
    "def plot_solution(J,u,label_name):\n",
    "    N = 2 ** J -1\n",
    "    h = 1/2**J\n",
    "    X = np.arange(h, 1, h)\n",
    "    Y = np.arange(h, 1, h)\n",
    "    X, Y = np.meshgrid(X,Y)   # create a mesh\n",
    "    a = torch.reshape(u, (N, N))\n",
    "    fig1 = plt.figure()\n",
    "    ax = Axes3D(fig1)         # plot a 3D surface, (X,Y,u(X,Y))\n",
    "    ax.plot_surface(X, Y, np.array(a.data), rstride=1, cstride=1, cmap=plt.cm.coolwarm)\n",
    "    ax.set_title(label_name)\n",
    "\n",
    "    \n",
    "def plot_error(M,error,label_name):\n",
    "    plt.figure()\n",
    "    plt.title('Error vs number of iterations using '+label_name)\n",
    "    plot = plt.plot(error)\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "    \n",
    "#Kernels setup\n",
    "RT = nn.ConvTranspose2d(1,1,kernel_size=3,stride=2,padding=0,bias=False)\n",
    "get_mg_init(None,None,None,None,RT)\n",
    "A = nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "get_mg_init(A,None,None,None,None)\n",
    "\n",
    "#MultiGrid Model Setup\n",
    "num_channel_input = 1\n",
    "num_channel_u = 1\n",
    "num_channel_f = 1\n",
    "num_classes = 1\n",
    "\n",
    "J = 4\n",
    "M = 100\n",
    "num_iteration = [2,2,2,2]\n",
    "\n",
    "MG0 = MG(num_channel_input, num_iteration, num_channel_u, num_channel_f,num_classes)\n",
    "\n",
    "N = 2**J - 1\n",
    "f = torch.ones(1,1,N,N)/(N+1)**2\n",
    "u = torch.randn(1,1,N,N)\n",
    "error = [np.linalg.norm((A(u)-f).detach().numpy())]\n",
    "start = timer()\n",
    "for m in range (M):\n",
    "    u_list = []\n",
    "    u = MG0(u,f)\n",
    "    \n",
    "    for j in range(J-1,0,-1):\n",
    "        \n",
    "        u_list[j] += RT(u_list[j+1])\n",
    "    \n",
    "    u = u_list[1]\n",
    "    \n",
    "    error.append(np.linalg.norm((A(u)-f).detach().numpy()))\n",
    "end = timer()\n",
    "print(\"Computation Time:\", end-start)\n",
    "plot_error(M, error, \"Multigrid\")\n",
    "plot_solution(J, u, \"Numerical Solution\")\n",
    "\n",
    "min_error = 10**-5\n",
    "u = torch.randn(1,1,N,N)             # initial value for u\n",
    "current_error = np.linalg.norm((A(u)-f).detach().numpy())\n",
    "iter_count = 0\n",
    "\n",
    "while(min_error<current_error):\n",
    "    u_list = []\n",
    "    u = MG0(u,f)\n",
    "    \n",
    "    for j in range(J-1,0,-1):    \n",
    "        u_list[j] += RT(u_list[j+1])\n",
    "    \n",
    "    u = u_list[1]\n",
    "    \n",
    "    current_error = np.linalg.norm((A(u)-f).detach().numpy())\n",
    "    iter_count+=1\n",
    "print(\"Number of iterations required to reach minimum error of\",min_error,\"is\",iter_count,\"iterations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13088/1517214181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;34m'''FEM GD'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;31m# cbook must import matplotlib only within function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;31m# definitions, so it is safe to import from it here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatplotlibDeprecationWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msanitize_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmplDeprecation\u001b[0m  \u001b[1;31m# deprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\rcsetup.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mls_mapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mColormap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_color_like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfontconfig_pattern\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparse_fontconfig_pattern\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enums\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJoinStyle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCapStyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\fontconfig_pattern.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m from pyparsing import (Literal, ZeroOrMore, Optional, Regex, StringEnd,\n\u001b[0m\u001b[0;32m     16\u001b[0m                        ParseException, Suppress)\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''FEM GD'''\n",
    "def FEM_GD(u,n,eta,A,f):\n",
    "    u1=u+eta*(f-A(u))\n",
    "    return u1\n",
    "'''MODEL SET-UP & TESTING '''\n",
    "#Plotting functions \n",
    "def plot_solution(J,u,label_name):\n",
    "    N = 2 ** J -1\n",
    "    h = 1/2**J\n",
    "    X = np.arange(h, 1, h)\n",
    "    Y = np.arange(h, 1, h)\n",
    "    X, Y = np.meshgrid(X,Y)   # create a mesh\n",
    "    a = torch.reshape(u, (N, N))\n",
    "    fig1 = plt.figure()\n",
    "    ax = Axes3D(fig1)         # plot a 3D surface, (X,Y,u(X,Y))\n",
    "    ax.plot_surface(X, Y, np.array(a.data), rstride=1, cstride=1, cmap=plt.cm.coolwarm)\n",
    "    ax.set_title(label_name)\n",
    "\n",
    "    \n",
    "def plot_error(M,error,label_name):\n",
    "    #print(np.linalg.norm((f-self.A(u)).reshape(-1).detach().numpy()))\n",
    "    plt.figure()\n",
    "    #print(\"plt title\")\n",
    "    #input()\n",
    "    plt.title('Error vs number of iterations using '+label_name)\n",
    "    #print(\"begin plot error 3\")\n",
    "    #input()\n",
    "    plot = plt.plot(error)\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "\n",
    "A = nn.Conv2d(1,1,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "A_kernel = torch.tensor([[[[0,-1,0],[-1,4,-1],[0,-1,0]]]],dtype=torch.float32)\n",
    "A.weight = torch.nn.Parameter(A_kernel)\n",
    "\n",
    "J = 4                                # grid level\n",
    "N = 2**J - 1                         # number of inner grid points\n",
    "f = torch.ones(1,1,N,N)/(N+1)**2     # real function\n",
    "eta = 1/8                            # learning rate\n",
    "u = torch.randn(1,1,N,N)             # initial value for u\n",
    "M = 100\n",
    " \n",
    "#(u,N,eta,A,f)\n",
    "error = [np.linalg.norm((A(u)-f).detach().numpy())]\n",
    "start = timer()\n",
    "for m in range (M):\n",
    "    u = FEM_GD(u,N,eta,A,f)\n",
    "    error.append(np.linalg.norm((A(u)-f).detach().numpy()))\n",
    "end = timer()\n",
    "print(\"Computation Time:\",end-start)\n",
    "plot_error(M, error, \"Gradient Descent\")\n",
    "plot_solution(J, u, \"Numerical Solution\")\n",
    "\n",
    "min_error = 10**-5\n",
    "u = torch.randn(1,1,N,N)             # initial value for u\n",
    "current_error = np.linalg.norm((A(u)-f).detach().numpy())\n",
    "iter_count = 0\n",
    "\n",
    "\n",
    "\n",
    "while(min_error<current_error):\n",
    "    u = FEM_GD(u,N,eta,A,f)\n",
    "    current_error = np.linalg.norm((A(u)-f).detach().numpy())\n",
    "    iter_count+=1\n",
    "print(\"Number of iterations required to reach minimum error of\",min_error,\"is\",iter_count,\"iterations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0S78NQv94Dj"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cczmyGLc94Dk"
   },
   "source": [
    "## Problem 2 [50%]: \n",
    "\n",
    "Use SGD with momentum and weight decay to train MgNet on the Cifar10 dataset. Use 120 epochs, set the initial learning rate to 0.1, momentum to 0.9, weight decay to 0.0005, and divide the learning rate by 10 every 30 epochs. (The code to do this has been provided.) Let $b_i$ denote the test accuracy of the model after $i$ epochs, and let $b^*$ = $\\max_i(b_i)$ be the best test accuracy attained during training.\n",
    "\n",
    "\n",
    "### Tasks:\n",
    "   * [30%] Train MgNet with the following three sets of hyper-parameters (As a reminder, the hyper-parameters of MgNet are $\\nu$, the number of iterations of each layer, $c_u$, the number of channels for $u$, and $c_f$, the number of channels for $f$.):\n",
    " \n",
    "    (1) $\\nu=$[1,1,1,1], $c_u=c_f=64$.\n",
    "    \n",
    "    (2) $\\nu=$[2,2,2,2], $c_u=c_f=64$.\n",
    "\n",
    "    (3) $\\nu=$[2,2,2,2], $c_u=c_f=64$, try to improve the test accuracy by implementing MgNet with $S^{l,i}$, which means different iterations in the same layer do not share the same $S^{l}$. \n",
    "  \n",
    "  \n",
    "   * For each numerical experiment above, print the results with the following format:\n",
    "\n",
    "       \"Epoch: i, Learning rate: lr$_i$, Training accuracy: $a_i$, Test accuracy: $b_i$\"\n",
    "\n",
    "        where $i=1,2,3,...$ means the $i$-th epoch,  $a_i$ and $b_i$ are the training accuracy and test accuracy computed at the end of $i$-th epoch, and lr$_i$ is the learning rate of $i$-th epoch.\n",
    "    \n",
    "    \n",
    "   * [10%] For each numerical experiment above, plot the test accuracy against the epoch count, i.e. the x-axis is the number of epochs $i$ and y-axis is the test accuracy $b_i$. An example plot is shown in the next cell.\n",
    "   \n",
    "   \n",
    "   * [10%] Calculate the number of parameters that each of the above models has. Discuss why the number of parameters is different (or the same) for each of the models.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Use GPU?', use_cuda)\n",
    "\n",
    "class MgIte(nn.Module):\n",
    "    def __init__(self, A, S):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.A = A\n",
    "        self.S = S\n",
    "\n",
    "        self.bn1 =nn.BatchNorm2d(A.weight.size(0)) ##### For MgNet: BN #####\n",
    "        self.bn2 =nn.BatchNorm2d(S.weight.size(0)) ##### For MgNet: BN #####\n",
    "    \n",
    "    def forward(self, out):\n",
    "        u, f = out\n",
    "\n",
    "        u = u + F.relu(self.bn2(self.S(F.relu(self.bn1((f-self.A(u))))))) ##### For MgNet: add BN and ReLU #####\n",
    "        out = (u, f)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MgRestriction(nn.Module):\n",
    "    def __init__(self, A_old, A, Pi, R):\n",
    "        super().__init__()\n",
    "\n",
    "        self.A_old = A_old\n",
    "        self.A = A\n",
    "        self.Pi = Pi\n",
    "        self.R = R\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(Pi.weight.size(0))   ##### For MgNet: BN #####\n",
    "        self.bn2 = nn.BatchNorm2d(R.weight.size(0))    ##### For MgNet: BN #####\n",
    "\n",
    "    def forward(self, out):\n",
    "        u_old, f_old = out\n",
    "\n",
    "        u = F.relu(self.bn1(self.Pi(u_old)))                              ##### For MgNet: add BN and ReLU #####\n",
    "        f = F.relu(self.bn2(self.R(f_old-self.A_old(u_old))))+self.A(u)\n",
    "        \n",
    "        out = (u,f)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MgNet(nn.Module):\n",
    "    def __init__(self, num_channel_input, num_iteration, num_channel_u, num_channel_f, num_classes, s_li):\n",
    "        super().__init__()\n",
    "        self.num_iteration = num_iteration\n",
    "        self.num_channel_u = num_channel_u\n",
    "        \n",
    "        ##### For MgNet: Initialization layer #####\n",
    "        self.conv1 = nn.Conv2d(num_channel_input, num_channel_f, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channel_f)        \n",
    "\n",
    "        \n",
    "        A = nn.Conv2d(num_channel_u, num_channel_f, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "        layers = []\n",
    "        for l, num_iteration_l in enumerate(num_iteration): #l: l-th layer.   num_iteration_l: the number of iterations of l-th layer\n",
    "            for i in range(num_iteration_l):\n",
    "                if(s_li):\n",
    "                    S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "                layers.append(MgIte(A, S))\n",
    "\n",
    "            setattr(self, 'layer'+str(l), nn.Sequential(*layers))\n",
    "            # set attribute. This is equivalent to define\n",
    "            # self.layer1 = nn.Sequential(*layers)\n",
    "            # self.layer2 = nn.Sequential(*layers)\n",
    "            # ...\n",
    "            # self.layerJ = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "            if l < len(num_iteration)-1:\n",
    "                A_old = A\n",
    "\n",
    "                A = nn.Conv2d(num_channel_u, num_channel_f, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "                S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "                \n",
    "                ##### For MgNet: padding=1 #####\n",
    "                Pi = nn.Conv2d(num_channel_u, num_channel_u, kernel_size=3,stride=2, padding=1, bias=False)\n",
    "                R = nn.Conv2d(num_channel_f, num_channel_f, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "                \n",
    "                \n",
    "                layers= [MgRestriction(A_old, A, Pi, R)]\n",
    "        \n",
    "        ##### For MgNet: average pooling and fully connected layer for classification #####\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)  # pooling the data in each channel to size=1\n",
    "        self.fc = nn.Linear(num_channel_u ,num_classes)\n",
    "\n",
    "    def forward(self, u, f):\n",
    "        f = F.relu(self.bn1(self.conv1(f)))                 ##### For MgNet: initialization of f #####\n",
    "        if use_cuda:                                        ##### For MgNet: initialization of u #####\n",
    "            u = torch.zeros(f.size(0),self.num_channel_u,f.size(2),f.size(3), device=torch.device('cuda'))\n",
    "        else:\n",
    "            u = torch.zeros(f.size(0),self.num_channel_u,f.size(2),f.size(3))        \n",
    "        \n",
    "        out = (u, f) \n",
    "\n",
    "        for l in range(len(self.num_iteration)):\n",
    "            out = getattr(self, 'layer'+str(l))(out) \n",
    "            u, f = out                                        ##### For MG: save u^j, j=1,2,...,J #####\n",
    "        ##### For MgNet: average pooling and fully connected layer for classification #####\n",
    "        u = self.pooling(u)\n",
    "        u = u.view(u.shape[0], -1)\n",
    "        u = self.fc(u)\n",
    "        return u                                        \n",
    "    \n",
    "def adjust_learning_rate(optimizer, epoch, init_lr):\n",
    "    #lr = 1.0 / (epoch + 1)\n",
    "    lr = init_lr * 0.1 ** (epoch // 30)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def test_model(my_model, num_epochs):\n",
    "    minibatch_size = 128\n",
    "    \n",
    "    lr = 0.1\n",
    "    \n",
    "    if use_cuda:\n",
    "        my_model = my_model.cuda()\n",
    "\n",
    "    # Step 2: Define a loss function and training algorithm\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(my_model.parameters(), lr=lr, momentum=0.9, weight_decay = 0.0005)\n",
    "\n",
    "\n",
    "    # Step 3: load dataset\n",
    "    normalize = torchvision.transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    transform_train = torchvision.transforms.Compose([torchvision.transforms.RandomCrop(32, padding=4),\n",
    "                                                      torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                                      torchvision.transforms.ToTensor(),\n",
    "                                                      normalize])\n",
    "\n",
    "    transform_test  = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),normalize])\n",
    "\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=minibatch_size, shuffle=False)\n",
    "\n",
    "    test_acc = []\n",
    "    # classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    start = timer()\n",
    "\n",
    "    #Step 4: Train the NNs\n",
    "    # One epoch is when an entire dataset is passed through the neural network only once.\n",
    "    for epoch in range(num_epochs):\n",
    "        start_epoch = timer()\n",
    "\n",
    "        current_lr = adjust_learning_rate(optimizer, epoch, lr)\n",
    "\n",
    "        start_training = timer()\n",
    "        my_model.train()\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            if use_cuda:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass to get the loss\n",
    "            outputs = my_model(0,images)   # We need additional 0 input for u in MgNet\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and compute the gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  #backpropragation\n",
    "            optimizer.step() #update the weights/parameters\n",
    "        end_training = timer()\n",
    "        print('Computation Time for training:',end_training - start_training)\n",
    "\n",
    "      # Training accuracy\n",
    "        start_training_acc = timer()\n",
    "        my_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            with torch.no_grad():\n",
    "                if use_cuda:\n",
    "                    images = images.cuda()\n",
    "                    labels = labels.cuda()  \n",
    "                outputs = my_model(0,images)  # We need additional 0 input for u in MgNet\n",
    "                p_max, predicted = torch.max(outputs, 1) \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "        training_accuracy = float(correct)/total\n",
    "        end_training_acc = timer()\n",
    "        print('Computation Time for training accuracy:',end_training_acc - start_training_acc)\n",
    "\n",
    "\n",
    "        # Test accuracy\n",
    "        start_test_acc = timer()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(testloader):\n",
    "            with torch.no_grad():\n",
    "                if use_cuda:\n",
    "                    images = images.cuda()\n",
    "                    labels = labels.cuda()  \n",
    "                outputs = my_model(0,images)  # We need additional 0 input for u in MgNet\n",
    "                p_max, predicted = torch.max(outputs, 1) \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "        test_accuracy = float(correct)/total\n",
    "        test_acc.append(test_accuracy)\n",
    "        end_test_acc = timer()\n",
    "        print('Computation Time for test accuracy:',end_test_acc - start_test_acc)\n",
    "\n",
    "        print('Epoch: {}, learning rate: {}, the training accuracy: {}, the test accuracy: {}' .format(epoch+1,current_lr,training_accuracy,test_accuracy)) \n",
    "        end_epoch = timer()\n",
    "        print('Computation Time for one epoch:',end_epoch - start_epoch)\n",
    "\n",
    "    end = timer()\n",
    "    print('Total Computation Time:',end - start)\n",
    "    return test_acc\n",
    "num_channel_input = 3\n",
    "c_u = 64\n",
    "c_f = 64\n",
    "num_classes = 10 \n",
    "\n",
    "num_epochs = 120\n",
    "\n",
    "v = [[1,1,1,1],[2,2,2,2],[2,2,2,2]]\n",
    "s_li = [False,False,True]\n",
    "\n",
    "model_sizes = []\n",
    "test_accs = []\n",
    "# Step 1: Define a model\n",
    "for i in range(len(v)):\n",
    "    v_i = v[i]\n",
    "    s_lii = s_li[i]\n",
    "    my_model = MgNet(num_channel_input, v_i, c_u, c_f, num_classes,s_lii)\n",
    "    model_size = sum(param.numel() for param in my_model.parameters())\n",
    "    \n",
    "    test_acc = test_model(my_model, num_epochs)\n",
    "    \n",
    "    model_sizes.append(model_size)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "epochs = np.arange(0,num_epochs)\n",
    "\n",
    "f, axs = plt.subplots(2, 2, sharey=True)\n",
    "\n",
    "axs[0,0].set_title('(1) v = [1,1,1,1]')\n",
    "axs[0,0].plot(epochs, test_accs[0])\n",
    "\n",
    "axs[0,1].set_title('(2) v = [2,2,2,2]')\n",
    "axs[0,1].plot(epochs, test_accs[1])\n",
    "\n",
    "\n",
    "axs[1,0].set_title('(3) v = [2,2,2,2] and different S per layer')\n",
    "axs[1,0].plot(epochs, test_accs[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hbaPslU94Du"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFzRMwsB94Dv"
   },
   "source": [
    "## Problem 3 [25 %]:\n",
    "\n",
    "Try to improve the MgNet Accuracy by increasing the number of channels. (We use the same notation as in the previous problem.) Double the number of channels to $c_u=c_f=128$ and try different $\\nu$ to maximize the test accuracy.\n",
    "\n",
    "### Tasks:\n",
    "   * [20%] Report $b^{*}$, $\\nu$ and the number of parameters of your model for each of the experiments you run.\n",
    "   * [5%] For the best experiment, plot the test accuracy against the epoch count, i.e. the x-axis is the number of epochs $i$ and y-axis is the test accuracy $b_i$. (Same as for the previous problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h07KdcUz94Dv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Use GPU?', use_cuda)\n",
    "\n",
    "class MgIte(nn.Module):\n",
    "    def __init__(self, A, S):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.A = A\n",
    "        self.S = S\n",
    "\n",
    "        self.bn1 =nn.BatchNorm2d(A.weight.size(0)) ##### For MgNet: BN #####\n",
    "        self.bn2 =nn.BatchNorm2d(S.weight.size(0)) ##### For MgNet: BN #####\n",
    "    \n",
    "    def forward(self, out):\n",
    "        u, f = out\n",
    "\n",
    "        u = u + F.relu(self.bn2(self.S(F.relu(self.bn1((f-self.A(u))))))) ##### For MgNet: add BN and ReLU #####\n",
    "        out = (u, f)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MgRestriction(nn.Module):\n",
    "    def __init__(self, A_old, A, Pi, R):\n",
    "        super().__init__()\n",
    "\n",
    "        self.A_old = A_old\n",
    "        self.A = A\n",
    "        self.Pi = Pi\n",
    "        self.R = R\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(Pi.weight.size(0))   ##### For MgNet: BN #####\n",
    "        self.bn2 = nn.BatchNorm2d(R.weight.size(0))    ##### For MgNet: BN #####\n",
    "\n",
    "    def forward(self, out):\n",
    "        u_old, f_old = out\n",
    "\n",
    "        u = F.relu(self.bn1(self.Pi(u_old)))                              ##### For MgNet: add BN and ReLU #####\n",
    "        f = F.relu(self.bn2(self.R(f_old-self.A_old(u_old))))+self.A(u)\n",
    "        \n",
    "        out = (u,f)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MgNet(nn.Module):\n",
    "    def __init__(self, num_channel_input, num_iteration, num_channel_u, num_channel_f, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_iteration = num_iteration\n",
    "        self.num_channel_u = num_channel_u\n",
    "        \n",
    "        ##### For MgNet: Initialization layer #####\n",
    "        self.conv1 = nn.Conv2d(num_channel_input, num_channel_f, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channel_f)        \n",
    "\n",
    "        \n",
    "        A = nn.Conv2d(num_channel_u, num_channel_f, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "        layers = []\n",
    "        for l, num_iteration_l in enumerate(num_iteration): #l: l-th layer.   num_iteration_l: the number of iterations of l-th layer\n",
    "            for i in range(num_iteration_l):\n",
    "                S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "                layers.append(MgIte(A, S))\n",
    "\n",
    "            setattr(self, 'layer'+str(l), nn.Sequential(*layers))\n",
    "            # set attribute. This is equivalent to define\n",
    "            # self.layer1 = nn.Sequential(*layers)\n",
    "            # self.layer2 = nn.Sequential(*layers)\n",
    "            # ...\n",
    "            # self.layerJ = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "            if l < len(num_iteration)-1:\n",
    "                A_old = A\n",
    "\n",
    "                A = nn.Conv2d(num_channel_u, num_channel_f, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "                S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "                \n",
    "                ##### For MgNet: padding=1 #####\n",
    "                Pi = nn.Conv2d(num_channel_u, num_channel_u, kernel_size=3,stride=2, padding=1, bias=False)\n",
    "                R = nn.Conv2d(num_channel_f, num_channel_f, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "                \n",
    "                \n",
    "                layers= [MgRestriction(A_old, A, Pi, R)]\n",
    "        \n",
    "        ##### For MgNet: average pooling and fully connected layer for classification #####\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)  # pooling the data in each channel to size=1\n",
    "        self.fc = nn.Linear(num_channel_u ,num_classes)\n",
    "\n",
    "    def forward(self, u, f):\n",
    "        f = F.relu(self.bn1(self.conv1(f)))                 ##### For MgNet: initialization of f #####\n",
    "        if use_cuda:                                        ##### For MgNet: initialization of u #####\n",
    "            u = torch.zeros(f.size(0),self.num_channel_u,f.size(2),f.size(3), device=torch.device('cuda'))\n",
    "        else:\n",
    "            u = torch.zeros(f.size(0),self.num_channel_u,f.size(2),f.size(3))        \n",
    "        \n",
    "        out = (u, f) \n",
    "\n",
    "        for l in range(len(self.num_iteration)):\n",
    "            out = getattr(self, 'layer'+str(l))(out) \n",
    "            u, f = out                                        ##### For MG: save u^j, j=1,2,...,J #####\n",
    "        ##### For MgNet: average pooling and fully connected layer for classification #####\n",
    "        u = self.pooling(u)\n",
    "        u = u.view(u.shape[0], -1)\n",
    "        u = self.fc(u)\n",
    "        return u                                        \n",
    "    \n",
    "def adjust_learning_rate(optimizer, epoch, init_lr):\n",
    "    #lr = 1.0 / (epoch + 1)\n",
    "    lr = init_lr * 0.1 ** (epoch // 30)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def test_model(my_model, num_epochs):\n",
    "    minibatch_size = 128\n",
    "    lr = 0.1\n",
    "    \n",
    "    if use_cuda:\n",
    "        my_model = my_model.cuda()\n",
    "\n",
    "    # Step 2: Define a loss function and training algorithm\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(my_model.parameters(), lr=lr, momentum=0.9, weight_decay = 0.0005)\n",
    "\n",
    "\n",
    "    # Step 3: load dataset\n",
    "    normalize = torchvision.transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    transform_train = torchvision.transforms.Compose([torchvision.transforms.RandomCrop(32, padding=4),\n",
    "                                                      torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                                      torchvision.transforms.ToTensor(),\n",
    "                                                      normalize])\n",
    "\n",
    "    transform_test  = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),normalize])\n",
    "\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=minibatch_size, shuffle=False)\n",
    "\n",
    "    test_acc = []\n",
    "    # classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    start = timer()\n",
    "\n",
    "    #Step 4: Train the NNs\n",
    "    # One epoch is when an entire dataset is passed through the neural network only once.\n",
    "    for epoch in range(num_epochs):\n",
    "        start_epoch = timer()\n",
    "\n",
    "        current_lr = adjust_learning_rate(optimizer, epoch, lr)\n",
    "\n",
    "        start_training = timer()\n",
    "        my_model.train()\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            if use_cuda:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass to get the loss\n",
    "            outputs = my_model(0,images)   # We need additional 0 input for u in MgNet\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and compute the gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  #backpropragation\n",
    "            optimizer.step() #update the weights/parameters\n",
    "        end_training = timer()\n",
    "        print('Computation Time for training:',end_training - start_training)\n",
    "\n",
    "      # Training accuracy\n",
    "        start_training_acc = timer()\n",
    "        my_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            with torch.no_grad():\n",
    "                if use_cuda:\n",
    "                    images = images.cuda()\n",
    "                    labels = labels.cuda()  \n",
    "                outputs = my_model(0,images)  # We need additional 0 input for u in MgNet\n",
    "                p_max, predicted = torch.max(outputs, 1) \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "        training_accuracy = float(correct)/total\n",
    "        end_training_acc = timer()\n",
    "        print('Computation Time for training accuracy:',end_training_acc - start_training_acc)\n",
    "\n",
    "\n",
    "        # Test accuracy\n",
    "        start_test_acc = timer()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(testloader):\n",
    "            with torch.no_grad():\n",
    "                if use_cuda:\n",
    "                    images = images.cuda()\n",
    "                    labels = labels.cuda()  \n",
    "                outputs = my_model(0,images)  # We need additional 0 input for u in MgNet\n",
    "                p_max, predicted = torch.max(outputs, 1) \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "        test_accuracy = float(correct)/total\n",
    "        test_acc.append(test_accuracy)\n",
    "        end_test_acc = timer()\n",
    "        print('Computation Time for test accuracy:',end_test_acc - start_test_acc)\n",
    "\n",
    "        print('Epoch: {}, learning rate: {}, the training accuracy: {}, the test accuracy: {}' .format(epoch+1,current_lr,training_accuracy,test_accuracy)) \n",
    "        end_epoch = timer()\n",
    "        print('Computation Time for one epoch:',end_epoch - start_epoch)\n",
    "\n",
    "    end = timer()\n",
    "    print('Total Computation Time:',end - start)\n",
    "    return test_acc\n",
    "\n",
    "num_epochs = 120\n",
    "\n",
    "num_channel_input = 3\n",
    "c_u = 128\n",
    "c_f = 128\n",
    "num_classes = 10 \n",
    "v = [[1,1,1,1],[2,2,2,2],[1,1,2,2],[1,2,1,2]]\n",
    "\n",
    "model_sizes = []\n",
    "test_accs = []\n",
    "# Step 1: Define a model\n",
    "for v_i in v:\n",
    "    my_model = MgNet(num_channel_input, v_i, c_u, c_f, num_classes)\n",
    "    model_size = sum(param.numel() for param in my_model.parameters())\n",
    "    \n",
    "    test_acc = test_model(my_model, num_epochs)\n",
    "    \n",
    "    model_sizes.append(model_size)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "epochs = np.arange(0,num_epochs)\n",
    "\n",
    "\n",
    "max_acc = 0\n",
    "max_acc_i = 0\n",
    "print(\"================================================================================================================\")\n",
    "for i in range(len(test_accs)):\n",
    "    max_t_i = max(test_accs[i])\n",
    "    v_i = v[i]\n",
    "    modelsize = model_sizes[i]\n",
    "    print(\"b* =\",max_t_i,\", v =\", v_i, \" model size =\", modelsize)\n",
    "\n",
    "\n",
    "for i in range(len(test_accs)):\n",
    "    t_i = test_accs[i]\n",
    "    max_ti = max(t_i)\n",
    "    \n",
    "    if(max_ti>max_acc):\n",
    "        max_acc = max_ti\n",
    "        max_acc_i = i\n",
    "\n",
    "print(\"Value v =\",v[max_acc_i], \"led to highest test accuracy:\",max_acc)\n",
    "plt.title(\"Max Accuracy\")\n",
    "plt.plot(np.arange(0, num_epochs),test_accs[max_acc_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NqbTlxG94D0"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGI1VtYL94D0"
   },
   "source": [
    "## Problem 4 [5%]:\n",
    "\n",
    "Continue testing larger MgNet models (i.e. increase the number of channels) to maximize the test accuracy. (Again, we use the same notation as in problem 2.)\n",
    "\n",
    "### Tasks:    \n",
    "    \n",
    "+  [5%] Try different training strategies and MgNet architectures with the goal of achieving $b^*>$ 95%. Hint: you can tune the number of epochs, the learning rate schedule, $c_u$, $c_f$, $\\nu$, try different $S^{l,i}$ in the same layer $l$, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Use GPU?', use_cuda)\n",
    "\n",
    "class MgIte(nn.Module):\n",
    "    def __init__(self, A, S):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.A = A\n",
    "        self.S = S\n",
    "\n",
    "        self.bn1 =nn.BatchNorm2d(A.weight.size(0)) ##### For MgNet: BN #####\n",
    "        self.bn2 =nn.BatchNorm2d(S.weight.size(0)) ##### For MgNet: BN #####\n",
    "    \n",
    "    def forward(self, out):\n",
    "        u, f = out\n",
    "\n",
    "        u = u + F.relu(self.bn2(self.S(F.relu(self.bn1((f-self.A(u))))))) ##### For MgNet: add BN and ReLU #####\n",
    "        out = (u, f)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MgRestriction(nn.Module):\n",
    "    def __init__(self, A_old, A, Pi, R):\n",
    "        super().__init__()\n",
    "\n",
    "        self.A_old = A_old\n",
    "        self.A = A\n",
    "        self.Pi = Pi\n",
    "        self.R = R\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(Pi.weight.size(0))   ##### For MgNet: BN #####\n",
    "        self.bn2 = nn.BatchNorm2d(R.weight.size(0))    ##### For MgNet: BN #####\n",
    "\n",
    "    def forward(self, out):\n",
    "        u_old, f_old = out\n",
    "\n",
    "        u = F.relu(self.bn1(self.Pi(u_old)))                              ##### For MgNet: add BN and ReLU #####\n",
    "        f = F.relu(self.bn2(self.R(f_old-self.A_old(u_old))))+self.A(u)\n",
    "        \n",
    "        out = (u,f)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MgNet(nn.Module):\n",
    "    def __init__(self, num_channel_input, num_iteration, num_channel_u, num_channel_f, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_iteration = num_iteration\n",
    "        self.num_channel_u = num_channel_u\n",
    "        \n",
    "        ##### For MgNet: Initialization layer #####\n",
    "        self.conv1 = nn.Conv2d(num_channel_input, num_channel_f, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channel_f)        \n",
    "\n",
    "        \n",
    "        A = nn.Conv2d(num_channel_u, num_channel_f, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "        layers = []\n",
    "        for l, num_iteration_l in enumerate(num_iteration): #l: l-th layer.   num_iteration_l: the number of iterations of l-th layer\n",
    "            for i in range(num_iteration_l):\n",
    "                S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "                layers.append(MgIte(A, S))\n",
    "\n",
    "            setattr(self, 'layer'+str(l), nn.Sequential(*layers))\n",
    "            # set attribute. This is equivalent to define\n",
    "            # self.layer1 = nn.Sequential(*layers)\n",
    "            # self.layer2 = nn.Sequential(*layers)\n",
    "            # ...\n",
    "            # self.layerJ = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "            if l < len(num_iteration)-1:\n",
    "                A_old = A\n",
    "\n",
    "                A = nn.Conv2d(num_channel_u, num_channel_f, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "                S = nn.Conv2d(num_channel_f, num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "                \n",
    "                ##### For MgNet: padding=1 #####\n",
    "                Pi = nn.Conv2d(num_channel_u, num_channel_u, kernel_size=3,stride=2, padding=1, bias=False)\n",
    "                R = nn.Conv2d(num_channel_f, num_channel_f, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "                \n",
    "                \n",
    "                layers= [MgRestriction(A_old, A, Pi, R)]\n",
    "        \n",
    "        ##### For MgNet: average pooling and fully connected layer for classification #####\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)  # pooling the data in each channel to size=1\n",
    "        self.fc = nn.Linear(num_channel_u ,num_classes)\n",
    "\n",
    "    def forward(self, u, f):\n",
    "        f = F.relu(self.bn1(self.conv1(f)))                 ##### For MgNet: initialization of f #####\n",
    "        if use_cuda:                                        ##### For MgNet: initialization of u #####\n",
    "            u = torch.zeros(f.size(0),self.num_channel_u,f.size(2),f.size(3), device=torch.device('cuda'))\n",
    "        else:\n",
    "            u = torch.zeros(f.size(0),self.num_channel_u,f.size(2),f.size(3))        \n",
    "        \n",
    "        out = (u, f) \n",
    "\n",
    "        for l in range(len(self.num_iteration)):\n",
    "            out = getattr(self, 'layer'+str(l))(out) \n",
    "            u, f = out                                        ##### For MG: save u^j, j=1,2,...,J #####\n",
    "        ##### For MgNet: average pooling and fully connected layer for classification #####\n",
    "        u = self.pooling(u)\n",
    "        u = u.view(u.shape[0], -1)\n",
    "        u = self.fc(u)\n",
    "        return u                                        \n",
    "    \n",
    "def adjust_learning_rate(optimizer, epoch, init_lr):\n",
    "    #lr = 1.0 / (epoch + 1)\n",
    "    lr = init_lr * 0.1 ** (epoch // 30)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def test_model(my_model):\n",
    "    minibatch_size = 128\n",
    "    lr = 0.1\n",
    "    \n",
    "    if use_cuda:\n",
    "        my_model = my_model.cuda()\n",
    "\n",
    "    # Step 2: Define a loss function and training algorithm\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(my_model.parameters(), lr=lr, momentum=0.9, weight_decay = 0.0005)\n",
    "\n",
    "\n",
    "    # Step 3: load dataset\n",
    "    normalize = torchvision.transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    transform_train = torchvision.transforms.Compose([torchvision.transforms.RandomCrop(32, padding=4),\n",
    "                                                      torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                                      torchvision.transforms.ToTensor(),\n",
    "                                                      normalize])\n",
    "\n",
    "    transform_test  = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),normalize])\n",
    "\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=minibatch_size, shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=minibatch_size, shuffle=False)\n",
    "\n",
    "    test_acc = []\n",
    "    test_accuracy = 0\n",
    "    target_accuracy = 0.95\n",
    "    # classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    start = timer()\n",
    "    epoch = 0\n",
    "    #Step 4: Train the NNs\n",
    "    # One epoch is when an entire dataset is passed through the neural network only once.\n",
    "    while(test_accuracy<target_accuracy):\n",
    "        start_epoch = timer()\n",
    "        \n",
    "        current_lr = adjust_learning_rate(optimizer, epoch, lr)\n",
    "\n",
    "        start_training = timer()\n",
    "        my_model.train()\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            if use_cuda:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass to get the loss\n",
    "            outputs = my_model(0,images)   # We need additional 0 input for u in MgNet\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and compute the gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  #backpropragation\n",
    "            optimizer.step() #update the weights/parameters\n",
    "        end_training = timer()\n",
    "        print('Computation Time for training:',end_training - start_training)\n",
    "\n",
    "      # Training accuracy\n",
    "        start_training_acc = timer()\n",
    "        my_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            with torch.no_grad():\n",
    "                if use_cuda:\n",
    "                    images = images.cuda()\n",
    "                    labels = labels.cuda()  \n",
    "                outputs = my_model(0,images)  # We need additional 0 input for u in MgNet\n",
    "                p_max, predicted = torch.max(outputs, 1) \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "        training_accuracy = float(correct)/total\n",
    "        end_training_acc = timer()\n",
    "        print('Computation Time for training accuracy:',end_training_acc - start_training_acc)\n",
    "\n",
    "\n",
    "        # Test accuracy\n",
    "        start_test_acc = timer()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(testloader):\n",
    "            with torch.no_grad():\n",
    "                if use_cuda:\n",
    "                    images = images.cuda()\n",
    "                    labels = labels.cuda()  \n",
    "                outputs = my_model(0,images)  # We need additional 0 input for u in MgNet\n",
    "                p_max, predicted = torch.max(outputs, 1) \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "        test_accuracy = float(correct)/total\n",
    "        test_acc.append(test_accuracy)\n",
    "        end_test_acc = timer()\n",
    "        print('Computation Time for test accuracy:',end_test_acc - start_test_acc)\n",
    "\n",
    "        print('Epoch: {}, learning rate: {}, the training accuracy: {}, the test accuracy: {}' .format(epoch+1,current_lr,training_accuracy,test_accuracy)) \n",
    "        end_epoch = timer()\n",
    "        print('Computation Time for one epoch:',end_epoch - start_epoch)\n",
    "        epoch+=1\n",
    "        \n",
    "    end = timer()\n",
    "    print('Total Computation Time:',end - start)\n",
    "    return test_acc\n",
    "\n",
    "num_channel_input = 3\n",
    "c_u = 128\n",
    "c_f = 128\n",
    "num_classes = 10 \n",
    "v = [3,3,3,3]\n",
    "\n",
    "test_accs = []\n",
    "test_acc = 0\n",
    "iterations = 0\n",
    "# Step 1: Define a model\n",
    "my_model = MgNet(num_channel_input, v, c_u, c_f, num_classes)\n",
    "    \n",
    "test_accs = test_model(my_model)\n",
    "    \n",
    "iterations = len(test_accs)\n",
    "\n",
    "model_size = sum(param.numel() for param in my_model.parameters())\n",
    "\n",
    "print(\"Model size:\",model_size)\n",
    "print(\"Max Accuracy:\",max(test_accs))\n",
    "\n",
    "plt.plot(np.arange(0, iterations),test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YeIdHNAx94D1"
   },
   "source": [
    "================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
